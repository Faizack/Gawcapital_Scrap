{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel('data/Keywords.xlsx',sheet_name=\"Wilmar International\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.amnesty.org/en/latest/news/2016/11...</td>\n",
       "      <td>www.amnesty.org › en › latestPalm Oil: Global ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.amnesty.org/en/wp-content/uploads/...</td>\n",
       "      <td>www.amnesty.org › en › wp-contentAMNESTY INTER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://news.mongabay.com/2023/12/a-decade-of-...</td>\n",
       "      <td>news.mongabay.com › 2023 › 12A decade of stopp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.wilmar-international.com/docs/defa...</td>\n",
       "      <td>www.wilmar-international.com › docs › default-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.eco-business.com/news/amnesty-inte...</td>\n",
       "      <td>www.eco-business.com › news › amnesty-internat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>https://e-recruitment.wilmar.co.id/</td>\n",
       "      <td>e-recruitment.wilmar.co.idHome Page - E-Recrui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>https://www.ingredientsnetwork.com/wilmar-inte...</td>\n",
       "      <td>www.ingredientsnetwork.com › wilmar-internatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>https://www.infrontanalytics.com/fe-EN/30304FN...</td>\n",
       "      <td>www.infrontanalytics.com › fe-EN › 30304FNWilm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>https://www.marketscreener.com/quote/stock/WIL...</td>\n",
       "      <td>www.marketscreener.com › quote › stockWilmar I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>https://sginvestors.io/analysts/research/2023/...</td>\n",
       "      <td>sginvestors.io › analysts › researchWilmar Int...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>975 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  link  \\\n",
       "0    https://www.amnesty.org/en/latest/news/2016/11...   \n",
       "1    https://www.amnesty.org/en/wp-content/uploads/...   \n",
       "2    https://news.mongabay.com/2023/12/a-decade-of-...   \n",
       "3    https://www.wilmar-international.com/docs/defa...   \n",
       "4    https://www.eco-business.com/news/amnesty-inte...   \n",
       "..                                                 ...   \n",
       "970                https://e-recruitment.wilmar.co.id/   \n",
       "971  https://www.ingredientsnetwork.com/wilmar-inte...   \n",
       "972  https://www.infrontanalytics.com/fe-EN/30304FN...   \n",
       "973  https://www.marketscreener.com/quote/stock/WIL...   \n",
       "974  https://sginvestors.io/analysts/research/2023/...   \n",
       "\n",
       "                                              headline  \n",
       "0    www.amnesty.org › en › latestPalm Oil: Global ...  \n",
       "1    www.amnesty.org › en › wp-contentAMNESTY INTER...  \n",
       "2    news.mongabay.com › 2023 › 12A decade of stopp...  \n",
       "3    www.wilmar-international.com › docs › default-...  \n",
       "4    www.eco-business.com › news › amnesty-internat...  \n",
       "..                                                 ...  \n",
       "970  e-recruitment.wilmar.co.idHome Page - E-Recrui...  \n",
       "971  www.ingredientsnetwork.com › wilmar-internatio...  \n",
       "972  www.infrontanalytics.com › fe-EN › 30304FNWilm...  \n",
       "973  www.marketscreener.com › quote › stockWilmar I...  \n",
       "974  sginvestors.io › analysts › researchWilmar Int...  \n",
       "\n",
       "[975 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>headline</th>\n",
       "      <th>article_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.amnesty.org/en/latest/news/2016/11...</td>\n",
       "      <td>www.amnesty.org › en › latestPalm Oil: Global ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.amnesty.org/en/wp-content/uploads/...</td>\n",
       "      <td>www.amnesty.org › en › wp-contentAMNESTY INTER...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://news.mongabay.com/2023/12/a-decade-of-...</td>\n",
       "      <td>news.mongabay.com › 2023 › 12A decade of stopp...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.wilmar-international.com/docs/defa...</td>\n",
       "      <td>www.wilmar-international.com › docs › default-...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  \\\n",
       "0  https://www.amnesty.org/en/latest/news/2016/11...   \n",
       "1  https://www.amnesty.org/en/wp-content/uploads/...   \n",
       "2  https://news.mongabay.com/2023/12/a-decade-of-...   \n",
       "3  https://www.wilmar-international.com/docs/defa...   \n",
       "\n",
       "                                            headline article_content  \n",
       "0  www.amnesty.org › en › latestPalm Oil: Global ...                  \n",
       "1  www.amnesty.org › en › wp-contentAMNESTY INTER...            None  \n",
       "2  news.mongabay.com › 2023 › 12A decade of stopp...                  \n",
       "3  www.wilmar-international.com › docs › default-...             NaN  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def is_pdf(url):\n",
    "    # Send a HEAD request to get the response headers\n",
    "    response = requests.head(url)\n",
    "    \n",
    "    # Get the content type from the response headers\n",
    "    content_type = response.headers.get('Content-Type', '')\n",
    "    \n",
    "    # Check if 'pdf' is present in the content type\n",
    "    return 'pdf' in content_type.lower()\n",
    "\n",
    "def is_pdf_or_website(url):\n",
    "    # Check if the URL ends with '.pdf'\n",
    "    if url.lower().endswith('.pdf'):\n",
    "        return True\n",
    "    return is_pdf(url)\n",
    "\n",
    "# Test the function\n",
    "url1 = \"https://www.wilmar-international.com/docs/default-source/default-document-library/sustainability/grievance/asa2151842016english_the-great-palm-oil-scandal.pdf?sfvrsn=d4630ec8_2\"\n",
    "url2 = \"https://www.wilmar-international.com\"\n",
    "\n",
    "print(is_pdf_or_website(url1))  # Output: True\n",
    "print(is_pdf_or_website(url2))  # Output: False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "\n",
    "def download_pdf(url: str, pdf_file_name: str) -> str:\n",
    "    \"\"\"Download PDF from given URL to local directory using proxy.\"\"\"\n",
    "    try:\n",
    "\n",
    "\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "        req = request.Request(url, headers=headers)\n",
    "        with request.urlopen(req) as response:\n",
    "            if response.status == 200:\n",
    "                current_path=f'{os.getcwd()}/pdf/'\n",
    "                filepath = os.path.join(current_path, pdf_file_name)\n",
    "                with open(filepath, 'wb') as pdf_object:\n",
    "                    pdf_object.write(response.read())\n",
    "                print(f'{pdf_file_name} was successfully saved!')\n",
    "                return filepath  # Return the path to the downloaded PDF file\n",
    "            else:\n",
    "                print(f'Uh oh! Could not download {pdf_file_name}, HTTP response status code: {response.status}')\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from PyPDF2 import PdfFileReader\n",
    "\n",
    "def scrape_links(df, pdf_dir='pdf/', json_dir='json/'):\n",
    "    os.makedirs(pdf_dir, exist_ok=True)\n",
    "    os.makedirs(json_dir, exist_ok=True)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        link = row['link']\n",
    "        file_name = link.split('/')[-1]  # Extract file name from the link\n",
    "        \n",
    "        if link.endswith('.pdf'):\n",
    "\n",
    "            pdf_source=download_pdf(link,file_name)\n",
    "\n",
    "            \n",
    "            # Update DataFrame with source of PDF\n",
    "            df.at[index, 'source'] = 'pdf'\n",
    "            df.at[index, 'source_path'] = pdf_source\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                response = requests.get(link)\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "                # Extract name and keywords\n",
    "                name = soup.title.string.strip()\n",
    "                keywords = [word.strip() for word in name.split()]\n",
    "\n",
    "                # Initialize content dictionary\n",
    "                content = {\n",
    "                    \"name\": name,\n",
    "                    \"keywords\": keywords,\n",
    "                    \"url\": link,\n",
    "                    \"content\": {\n",
    "                        \"headings\": [],\n",
    "                        \"span_tags\": [],\n",
    "                        \"p_tags\": []\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                # Extract headings, articles, and paragraphs\n",
    "                headings = soup.find_all(['h1', 'h2', 'h3'])\n",
    "                for heading in headings:\n",
    "                    heading_content = heading.get_text().strip()\n",
    "                    articles = []\n",
    "                    article = {\n",
    "                        \"content\": \"\",\n",
    "                        \"paragraphs\": []\n",
    "                    }\n",
    "                    next_element = heading.find_next_sibling()\n",
    "                    while next_element and next_element.name not in ['h1', 'h2', 'h3']:\n",
    "                        if next_element.name == 'p':\n",
    "                            article[\"paragraphs\"].append({\"content\": next_element.get_text().strip()})\n",
    "                        elif next_element.name in ['h1', 'h2', 'h3']:\n",
    "                            break\n",
    "                        else:\n",
    "                            article[\"content\"] += next_element.get_text().strip() + \" \"\n",
    "                        next_element = next_element.find_next_sibling()\n",
    "                    if article[\"content\"] or article[\"paragraphs\"]:\n",
    "                        articles.append(article)\n",
    "                    content[\"content\"][\"headings\"].append({\n",
    "                        \"tag\": heading.name,\n",
    "                        \"content\": heading_content,\n",
    "                        \"articles\": articles\n",
    "                    })\n",
    "\n",
    "                # Extract span tags\n",
    "                spans = soup.find_all('span')\n",
    "                for span in spans:\n",
    "                    content[\"content\"][\"span_tags\"].append({\"content\": span.get_text().strip()})\n",
    "\n",
    "                # Extract p tags\n",
    "                paragraphs = soup.find_all('p')\n",
    "                for paragraph in paragraphs:\n",
    "                    content[\"content\"][\"p_tags\"].append({\"content\": paragraph.get_text().strip()})\n",
    "\n",
    "                # Save scraped data to JSON\n",
    "                json_path = os.path.join(json_dir, f'{file_name}.json')\n",
    "                with open(json_path, 'w') as json_file:\n",
    "                    json.dump({\"data\": content}, json_file)\n",
    "\n",
    "                # Update DataFrame with source of JSON data\n",
    "                df.at[index, 'source'] = 'json'\n",
    "                df.at[index, 'source_path'] = json_path\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing link {link}: {e}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "# df = scrape_links(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASA2158192017ENGLISH.pdf was successfully saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing link https://www.wilmar-international.com/docs/default-source/default-document-library/sustainability/grievance/asa2151842016english_the-great-palm-oil-scandal.pdf?sfvrsn=d4630ec8_2: 'NoneType' object has no attribute 'string'\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "df3 = scrape_links(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests as rq\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import PyPDF2\n",
    "from urllib import request\n",
    "\n",
    "       \n",
    "def process_link(row):\n",
    "    link = row['link']\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "    if link.endswith('.pdf'):\n",
    "        # Step 1: If link points to a PDF, download and save it\n",
    "        file_name = os.path.basename(link)\n",
    "        pdf_dir = 'pdf'\n",
    "        os.makedirs(pdf_dir, exist_ok=True)\n",
    "        path_link=download_pdf(link, file_name)\n",
    "\n",
    "        return path_link\n",
    "    else:\n",
    "        # Step 2: If not a PDF, scrape article from webpage\n",
    "        response = rq.get(link,headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Find article content\n",
    "        article_content = soup.find_all('p')\n",
    "        # Extract text\n",
    "        article_text = ' '.join([para.get_text(strip=True) for para in article_content])\n",
    "        return article_text\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASA2158192017ENGLISH.pdf was successfully saved!\n",
      "                                                  link  \\\n",
      "0    https://www.amnesty.org/en/latest/news/2016/11...   \n",
      "1    https://www.amnesty.org/en/wp-content/uploads/...   \n",
      "2    https://news.mongabay.com/2023/12/a-decade-of-...   \n",
      "3    https://www.wilmar-international.com/docs/defa...   \n",
      "4    https://www.eco-business.com/news/amnesty-inte...   \n",
      "..                                                 ...   \n",
      "970                https://e-recruitment.wilmar.co.id/   \n",
      "971  https://www.ingredientsnetwork.com/wilmar-inte...   \n",
      "972  https://www.infrontanalytics.com/fe-EN/30304FN...   \n",
      "973  https://www.marketscreener.com/quote/stock/WIL...   \n",
      "974  https://sginvestors.io/analysts/research/2023/...   \n",
      "\n",
      "                                              headline article_content  \n",
      "0    www.amnesty.org › en › latestPalm Oil: Global ...                  \n",
      "1    www.amnesty.org › en › wp-contentAMNESTY INTER...            None  \n",
      "2    news.mongabay.com › 2023 › 12A decade of stopp...                  \n",
      "3    www.wilmar-international.com › docs › default-...             NaN  \n",
      "4    www.eco-business.com › news › amnesty-internat...             NaN  \n",
      "..                                                 ...             ...  \n",
      "970  e-recruitment.wilmar.co.idHome Page - E-Recrui...             NaN  \n",
      "971  www.ingredientsnetwork.com › wilmar-internatio...             NaN  \n",
      "972  www.infrontanalytics.com › fe-EN › 30304FNWilm...             NaN  \n",
      "973  www.marketscreener.com › quote › stockWilmar I...             NaN  \n",
      "974  sginvestors.io › analysts › researchWilmar Int...             NaN  \n",
      "\n",
      "[975 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\faiza\\AppData\\Local\\Temp\\ipykernel_4504\\1688855881.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['article_content'] = df1.apply(process_link, axis=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df1['article_content'] = df1.apply(process_link, axis=1)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df.to_csv('articles.csv', index=False)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\faiza\\AppData\\Local\\Temp\\ipykernel_4504\\3838066016.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.at[index, 'source'] = 'json'\n",
      "C:\\Users\\faiza\\AppData\\Local\\Temp\\ipykernel_4504\\3838066016.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.at[index, 'source_path'] = json_path\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing link https://www.wilmar-international.com/docs/default-source/default-document-library/sustainability/grievance/asa2151842016english_the-great-palm-oil-scandal.pdf?sfvrsn=d4630ec8_2: 'NoneType' object has no attribute 'string'\n"
     ]
    }
   ],
   "source": [
    "df1 = scrape_links(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 403: Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://www.amnesty.org/en/latest/news/2016/11/palm-oil-global-brands-profiting-from-child-and-forced-labour/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[0;32m      3\u001b[0m    html \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      4\u001b[0m    \u001b[38;5;28mprint\u001b[39m(html)\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\urllib\\request.py:215\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\urllib\\request.py:521\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    520\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 521\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\urllib\\request.py:630\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 630\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\urllib\\request.py:559\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[0;32m    558\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\urllib\\request.py:492\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    491\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 492\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    494\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\urllib\\request.py:639\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 639\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "with urllib.request.urlopen('https://www.amnesty.org/en/latest/news/2016/11/palm-oil-global-brands-profiting-from-child-and-forced-labour/') as response:\n",
    "   html = response.read()\n",
    "   print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "\"data\":{\n",
    "    \"name\":\"Wilmar International\",\n",
    "    \"keywords\":[\n",
    "        \"Wilmar\",\n",
    "        \"International\",\n",
    "        \"Wilmar International\"\n",
    "    ],\n",
    "    \"url\":\"https://www.wilmarinternational.com/\",\n",
    "    \"content\":{\n",
    "        \"heading tag\":{\n",
    "            \"identifier\":\"identify the contain\",\n",
    "            \"content\":\"content of headings\"\n",
    "        },\n",
    "        \"article tag\":{\n",
    "            \"content\":\"content of articles\"\n",
    "        },\n",
    "        \"span tag\":{\n",
    "            \"content\":\"content of span\"\n",
    "        },\n",
    "        \"p tag\":{\n",
    "            \"content\":\"\"\n",
    "        },\n",
    "\n",
    "    }\n",
    "    \n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      https://www.amnesty.org/en/latest/news/2016/11...\n",
       "1      https://www.amnesty.org/en/wp-content/uploads/...\n",
       "2      https://news.mongabay.com/2023/12/a-decade-of-...\n",
       "3      https://www.wilmar-international.com/docs/defa...\n",
       "4      https://www.eco-business.com/news/amnesty-inte...\n",
       "                             ...                        \n",
       "970                  https://e-recruitment.wilmar.co.id/\n",
       "971    https://www.ingredientsnetwork.com/wilmar-inte...\n",
       "972    https://www.infrontanalytics.com/fe-EN/30304FN...\n",
       "973    https://www.marketscreener.com/quote/stock/WIL...\n",
       "974    https://sginvestors.io/analysts/research/2023/...\n",
       "Name: link, Length: 975, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['link']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
